from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

def evaluate_model(model, X_test, y_test):
    """
    Evaluate the given model.

    Parameters:
    model : estimator object
        The machine learning model to be evaluated.
    X_test : DataFrame or ndarray, shape (n_samples, n_features)
        Test feature data.
    y_test : Series or ndarray, shape (n_samples,)
        Test label data.

    Returns:
    report_dict : dict
        Classification report including precision, recall, and F1 score.
    report_str : str
        Classification report as a string including precision, recall, and F1 score.
    conf_matrix : ndarray
        Confusion matrix.
    """
    y_pred = model.predict(X_test)
    report_dict = classification_report(y_test, y_pred, output_dict=True)
    report_str = classification_report(y_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)
    return report_dict, report_str, conf_matrix

def plot_confusion_matrix(conf_matrix, class_names=None, file_path=None, figsize=(10, 7), cmap='YlOrRd', title='Confusion Matrix', fmt='d'):
    """
    Plot the confusion matrix.

    Parameters:
    conf_matrix : ndarray of shape (n_classes, n_classes)
        Confusion matrix.
    class_names : list of str, optional
        Class names. If None, they will be auto-generated from the confusion matrix.
    figsize : tuple of int, optional
        Size of the figure, default is (10, 7).
    cmap : str or matplotlib colormap, optional
        Color map, default is 'YlOrRd'.
    title : str, optional
        Title of the plot, default is 'Confusion Matrix'.
    fmt : str, optional
        Format string, default is 'd'.
    """
    if class_names is None:
        class_names = [str(i) for i in range(conf_matrix.shape[0])]

    plt.figure(figsize=figsize)
    sns.heatmap(conf_matrix, annot=True, fmt=fmt, cmap=cmap, xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted class')
    plt.ylabel('Actual class')
    plt.title(title)
    if file_path:
        plt.savefig(file_path)
        plt.close()
    else:
        plt.show()

def plot_radar_chart(report, labels=None, file_path=None, title='Radar Chart'):
    """
    Plot a radar chart from the classification report.

    Parameters:
    report : dict
        Classification report generated by classification_report.
    labels : list of str, optional
        Labels of the metrics to be plotted, default is ['precision', 'recall', 'f1-score'].
    file_path : str, optional
        File path to save the image. If None, the image will be displayed.
    title : str, optional
        Title of the plot, default is 'Radar Chart'.
    """
    if labels is None:
        labels = ['precision', 'recall', 'f1-score']

    # Extract metrics
    metrics_dict = {}
    for label, metrics in report.items():
        if label not in ('accuracy', 'macro avg', 'weighted avg'):
            metrics_dict[label] = [metrics[metric] for metric in labels]

    num_vars = len(labels)
    angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()
    angles += angles[:1]

    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))

    for label, metrics in metrics_dict.items():
        values = metrics
        values += values[:1]
        ax.plot(angles, values, label=label)
        ax.fill(angles, values, alpha=0.25)

    ax.set_yticklabels([])
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(labels)

    plt.title(title)
    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))

    if file_path:
        plt.savefig(file_path)
    else:
        plt.show()
